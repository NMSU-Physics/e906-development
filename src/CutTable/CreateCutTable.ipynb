{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7bcdc39-6330-473f-8c5c-31daa1ad4727",
   "metadata": {},
   "source": [
    "# 1.0 Cut Table for DY Absolute Cross Section Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9e3d9-96e4-4d36-bced-6ced90c95361",
   "metadata": {},
   "source": [
    "## 1.1 Files used:\n",
    "### Data Files:\n",
    "    \"/Users/ckuruppu/Documents/NMSU-Physics/e906-development/ROOTFiles/Hugo/roadset57_70_R008_2111v42_tmp_noPhys.root\"\n",
    "### Monte-Carlo Files:\n",
    "    \"/Users/ckuruppu/Documents/NMSU-Physics/e906-development/ROOTFiles/Hugo/mc_drellyan_LH2_M027_S001_messy_occ_pTxFweight_v2.root\"\n",
    "    \"/Users/ckuruppu/Documents/NMSU-Physics/e906-development/ROOTFiles/Hugo/mc_jpsi_LH2_M027_S001_messy_occ_pTxFweight_v2.root\"\n",
    "    \"/Users/ckuruppu/Documents/NMSU-Physics/e906-development/ROOTFiles/Hugo/mc_psiprime_LH2_M027_S001_messy_occ_pTxFweight_v2.root\"\n",
    "\n",
    "### Mixed Files:\n",
    "    \"/Users/ckuruppu/Documents/NMSU-Physics/e906-development/ROOTFiles/Hugo/merged_RS67_3089LH2.root\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1d4983",
   "metadata": {},
   "source": [
    "## 1.2 Todo:\n",
    "### 1.2.1 Add Data (RS67) from the mixed ROOT file (TTree name: result)\n",
    "### 1.2.2 Apply appropriate reweight (ReWeight) for MC events\n",
    "### 1.2.3 Add remaining roadsets (only received RS70 so far) to account mixed events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec3b451-d36a-435a-982a-ff7b1804c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Missing variables in ../../ROOTFiles/MixedEvents/merged_RS67_3089LH2.root: ['intensityP']. These will be added as NaN.\n",
      "⚠️ Missing variables in ../../ROOTFiles/MixedEvents/merged_RS67_3089LH2.root: ['intensityP']. These will be added as NaN.\n",
      "⚠️ Missing variables in ../../ROOTFiles/Hugo/mc_drellyan_LH2_M027_S001_messy_occ_pTxFweight_v2.root: ['intensityP']. These will be added as NaN.\n",
      "⚠️ Missing variables in ../../ROOTFiles/Hugo/mc_jpsi_LH2_M027_S001_messy_occ_pTxFweight_v2.root: ['intensityP']. These will be added as NaN.\n",
      "⚠️ Missing variables in ../../ROOTFiles/Hugo/mc_psiprime_LH2_M027_S001_messy_occ_pTxFweight_v2.root: ['intensityP']. These will be added as NaN.\n",
      "╒═════════════════════════════╤════════╤══════════════╤═══════════════╤══════════╤════════════╤════════════════╤════════════╤══════════════════╤══════════════════════╕\n",
      "│ Cut                         │   Data │   Data(RS67) │   Mixed(RS67) │    DY MC │   J/Psi MC │   Psi Prime MC │   Total MC │   Purity (DY MC) │   Efficiency (DY MC) │\n",
      "╞═════════════════════════════╪════════╪══════════════╪═══════════════╪══════════╪════════════╪════════════════╪════════════╪══════════════════╪══════════════════════╡\n",
      "│ Total Events                │ 614663 │     16517271 │       7945229 │ 19093900 │   60526336 │        6666208 │   86286448 │            22.13 │               100    │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ nhits1 + nhits2 > 29        │ 614663 │     16191722 │       7768630 │ 18821968 │   59839112 │        6596496 │   85257576 │            22.08 │                98.58 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ nhits1st1 + nhits2st1 > 8   │ 614663 │     15936907 │       7653785 │ 18694676 │   59586104 │        6570602 │   84851384 │            22.03 │                97.91 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ chisq dimuon < 18           │ 614663 │     10636888 │       4752043 │ 17291216 │   56534992 │        6246022 │   80072232 │            21.59 │                90.56 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ chisq Target within 2       │ 614663 │       412300 │        192954 │ 14880651 │   41357444 │        5287561 │   61525656 │            24.19 │                77.93 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ dx within -0.25 to 0.25     │ 614663 │       314041 │        131027 │ 14648834 │   39442396 │        5210330 │   59301564 │            24.7  │                76.72 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ dz within -280 to -5        │ 614663 │       308081 │        127346 │ 14642167 │   39418396 │        5206895 │   59267460 │            24.71 │                76.69 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ abs(trackSeparation) < 270  │ 614663 │       272421 │        105509 │ 14562776 │   38841908 │        5171338 │   58576024 │            24.86 │                76.27 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ abs(dpx) < 1.8              │ 614663 │       249665 │         93431 │ 14515724 │   38824216 │        5167031 │   58506972 │            24.81 │                76.02 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ abs(dpy) < 2                │ 614663 │       244802 │         90968 │ 14369734 │   38263500 │        5131828 │   57765060 │            24.88 │                75.26 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ dpz within 38 to 116        │ 614663 │       220823 │         72549 │ 14354176 │   38100328 │        5122357 │   57576860 │            24.93 │                75.18 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ dpx^2 + dpy^2 < 5           │ 614663 │       219393 │         71820 │ 14339352 │   38078964 │        5120772 │   57539088 │            24.92 │                75.1  │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ mass within 4.2 to 8.8      │ 143661 │        44731 │         13062 │  7925860 │      82999 │          93031 │    8101889 │            97.83 │                41.51 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ xF within -0.1 to 0.95      │ 143309 │        44176 │         12828 │  7884346 │      82999 │          93019 │    8060364 │            97.82 │                41.29 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ xT within -0.1 to 0.58      │ 143309 │        44176 │         12827 │  7884298 │      82999 │          93019 │    8060316 │            97.82 │                41.29 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ cosTheta within -0.5 to 0.5 │ 129477 │        30965 │          6116 │  7619733 │      49867 │          89905 │    7759505 │            98.2  │                39.91 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ D1 < 400                    │ 129236 │        30913 │          6088 │  7583295 │      49673 │          87936 │    7720904 │            98.22 │                39.72 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ D2 < 400                    │ 129186 │        30912 │          6088 │  7565660 │      46894 │          87813 │    7700366 │            98.25 │                39.62 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ D3 < 400                    │ 128928 │        30895 │          6085 │  7510494 │      46894 │          86995 │    7644382 │            98.25 │                39.33 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ D1 + D2 + D3 < 1000         │ 128715 │        30867 │          6077 │  7436450 │      24115 │          85190 │    7545755 │            98.55 │                38.95 │\n",
      "├─────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ intensity within 0 to 80000 │ 128631 │            0 │             0 │        0 │          0 │              0 │          0 │             0    │                 0    │\n",
      "╘═════════════════════════════╧════════╧══════════════╧═══════════════╧══════════╧════════════╧════════════════╧════════════╧══════════════════╧══════════════════════╛\n",
      "\n",
      "✅ Cut table saved as 'cut_table_with_mixed_and_reweighting.csv'\n"
     ]
    }
   ],
   "source": [
    "import uproot\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "\n",
    "def create_cut_table(data_file_path, mc_file_paths, mc_labels, tree_name, cuts, variables, mixed_file_path, mixed_tree_name):\n",
    "    \"\"\"\n",
    "    Creates a cut flow table by applying sequential cuts to data, MC, and mixed event samples.\n",
    "\n",
    "    Reads data from ROOT files, calculates a run-dependent beam offset, applies a\n",
    "    series of cuts, and reports the number of events remaining after each cut.\n",
    "    Includes calculations for total MC, purity, and efficiency for the DY MC sample.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_file_path : str\n",
    "        Path to the data ROOT file.\n",
    "    mc_file_paths : list of str\n",
    "        List of paths to the MC ROOT files.\n",
    "    mc_labels : list of str\n",
    "        List of labels corresponding to the MC files (used for column names).\n",
    "    tree_name : str\n",
    "        Name of the tree within the ROOT files containing the event data\n",
    "        (used for data and MC files).\n",
    "    cuts : dict\n",
    "        A dictionary where keys are descriptive cut names and values are\n",
    "        string expressions parsable by pandas.DataFrame.eval(). Cuts are applied\n",
    "        sequentially in the order they appear in the dictionary.\n",
    "    variables : list of str\n",
    "        List of variable names to read from the ROOT trees. This list should\n",
    "        include all variables used in the cut expressions, plus 'runID' and 'dy'.\n",
    "    mixed_file_path : str\n",
    "        Path to the file containing both Data(RS67) and Mixed(RS67) samples.\n",
    "    mixed_tree_name : str\n",
    "        Name of the tree within the mixed file containing the Mixed(RS67) data.\n",
    "        Assumes the Data(RS67) is in a tree named \"result\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or None\n",
    "        A pandas DataFrame representing the cut flow table with cut names as index\n",
    "        and samples (Data, MCs, Total MC, Mixed, Purity, Efficiency) as columns.\n",
    "        Returns None if a critical error occurs during data processing.\n",
    "    \"\"\"\n",
    "    cut_table = pd.DataFrame()\n",
    "\n",
    "    def apply_cuts(df, cuts, use_weights=True):\n",
    "        \"\"\"\n",
    "        Applies a dictionary of sequential cuts to a pandas DataFrame.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Input DataFrame containing event data.\n",
    "        cuts : dict\n",
    "            A dictionary of cut names and their corresponding string expressions.\n",
    "            Cuts are applied sequentially.\n",
    "        use_weights : bool, optional\n",
    "            Whether to use the 'ReWeight' column for summing events. Defaults to True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.Series\n",
    "            A pandas Series where the index is the cut name (including \"Total Events\")\n",
    "            and the value is the sum of event weights (or counts if no weights)\n",
    "            remaining after applying the cut and all previous cuts.\n",
    "        \"\"\"\n",
    "        cut_results = {}\n",
    "        # Use 'ReWeight' column if it exists and use_weights is True, otherwise use count of 1\n",
    "        weight = df['ReWeight'] if use_weights and 'ReWeight' in df.columns else pd.Series(1.0, index=df.index)\n",
    "\n",
    "        # Ensure weight is numeric\n",
    "        weight = pd.to_numeric(weight, errors='coerce').fillna(1.0)\n",
    "\n",
    "        cut_results[\"Total Events\"] = weight.sum()\n",
    "\n",
    "        # Ensure beamOffset column exists before cuts that might use it\n",
    "        if 'beamOffset' not in df.columns:\n",
    "             df['beamOffset'] = 0.0 # Default to 0 if calculation failed or column is missing\n",
    "\n",
    "        # Ensure dy is numeric before cuts that use it\n",
    "        if 'dy' in df.columns:\n",
    "             df['dy'] = pd.to_numeric(df['dy'], errors='coerce').fillna(np.nan)\n",
    "\n",
    "        # Keep track of the mask from successful cuts\n",
    "        cumulative_mask = pd.Series(True, index=df.index)\n",
    "\n",
    "\n",
    "        for cut_name, cut_string in cuts.items():\n",
    "            # Skip the total events entry if it somehow ended up in cuts\n",
    "            if cut_name == \"Total Events\":\n",
    "                 continue\n",
    "            try:\n",
    "                # Evaluate the cut string on the *current* filtered DataFrame\n",
    "                mask = df.eval(cut_string, engine='python')\n",
    "\n",
    "                # Apply the mask to the cumulative mask and the original weight series\n",
    "                cumulative_mask = cumulative_mask & mask\n",
    "\n",
    "                # Calculate the number of events remaining after this cut (using original weights)\n",
    "                cut_results[cut_name] = weight[cumulative_mask].sum()\n",
    "\n",
    "                # Filter the DataFrame and weight series for the *next* iteration\n",
    "                # We filter df here so subsequent cuts are applied to the reduced dataset\n",
    "                df = df[mask].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "                # Note: weight is *not* filtered here; we apply the cumulative_mask to the *original* weight series later\n",
    "                # This is important for correct sequential summing. The filtered 'df' is used for the *next* df.eval()\n",
    "\n",
    "            except Exception as e:\n",
    "                # This catch block now handles actual errors from df.eval(),\n",
    "                # e.g., if a column name used in the string is missing\n",
    "                print(f\"⚠️ Error applying cut '{cut_name}': {e}. Skipping cut for this dataset.\")\n",
    "                cut_results[cut_name] = 0\n",
    "                # If a cut fails, the cumulative_mask is NOT updated by this cut's mask.\n",
    "                # The dataframe 'df' is also NOT filtered by the failed mask.\n",
    "                # The next cut will be applied to the dataframe filtered only by *previous successful* cuts.\n",
    "\n",
    "\n",
    "        return pd.Series(cut_results)\n",
    "\n",
    "\n",
    "    def read_tree(file_path, tree_name, variables):\n",
    "        \"\"\"\n",
    "        Reads specified variables from a ROOT tree into a pandas DataFrame.\n",
    "\n",
    "        Handles missing files, trees, and variables gracefully by returning an empty\n",
    "        DataFrame on major errors or adding missing columns with NaN. Ensures\n",
    "        essential columns ('runID', 'dy', 'ReWeight') are always attempted and\n",
    "        converted to numeric types.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path : str\n",
    "            Path to the ROOT file.\n",
    "        tree_name : str\n",
    "            Name of the tree within the file.\n",
    "        variables : list of str\n",
    "            List of variable names to read.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            A pandas DataFrame containing the requested variables, plus 'runID', 'dy',\n",
    "            and 'ReWeight' if available. Missing columns are added as NaN. Returns\n",
    "            an empty DataFrame if the file or tree cannot be read.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with uproot.open(file_path) as file:\n",
    "                if tree_name not in file:\n",
    "                    print(f\"❌ Error: Tree '{tree_name}' not found in file '{file_path}'.\")\n",
    "                    # Return empty DataFrame with relevant columns for downstream processing structure\n",
    "                    empty_df = pd.DataFrame()\n",
    "                    # Include potentially required columns in empty df\n",
    "                    for var in set(variables + ['runID', 'dy', 'ReWeight']):\n",
    "                         empty_df[var] = pd.Series(dtype='float64') # Use float to accommodate NaN\n",
    "                    return empty_df\n",
    "\n",
    "\n",
    "                tree = file[tree_name]\n",
    "                all_keys = tree.keys()\n",
    "\n",
    "                # Ensure variables needed for beamOffset calculation and the cuts are requested\n",
    "                # Use a set to avoid duplicates, then convert back to list\n",
    "                vars_to_request = list(set(variables + ['runID', 'dy', 'ReWeight']))\n",
    "\n",
    "                vars_to_read = [var for var in vars_to_request if var in all_keys]\n",
    "                missing = [var for var in vars_to_request if var not in all_keys]\n",
    "                if missing:\n",
    "                    # Filter out 'ReWeight' from missing warning if it's likely data\n",
    "                    is_mc_file = any(mc_f in file_path for mc_f in mc_files) # Basic check\n",
    "                    if 'ReWeight' in missing and not is_mc_file:\n",
    "                         missing.remove('ReWeight')\n",
    "                         if missing:\n",
    "                             print(f\"⚠️ Missing variables in {file_path}: {missing}. These will be added as NaN.\")\n",
    "                         # else: print(f\"✅ All expected variables except ReWeight found in {file_path} (assuming data).\") # Optional verbose\n",
    "                    elif missing:\n",
    "                         print(f\"⚠️ Missing variables in {file_path}: {missing}. These will be added as NaN.\")\n",
    "\n",
    "\n",
    "                df = tree.arrays(vars_to_read, library=\"pd\")\n",
    "\n",
    "                # Add missing columns with NaN values to ensure DataFrame has expected structure\n",
    "                for var in missing:\n",
    "                     if var not in df.columns: # Avoid adding if it somehow got added during read\n",
    "                         df[var] = np.nan\n",
    "\n",
    "                # Ensure runID, dy, and ReWeight are numeric (important for beamOffset, cuts, and weights)\n",
    "                if 'runID' in df.columns:\n",
    "                    df['runID'] = pd.to_numeric(df['runID'], errors='coerce')\n",
    "                if 'dy' in df.columns:\n",
    "                    df['dy'] = pd.to_numeric(df['dy'], errors='coerce')\n",
    "                if 'ReWeight' in df.columns:\n",
    "                    # Fill NaN weights with 1.0 (no reweighting)\n",
    "                    df['ReWeight'] = pd.to_numeric(df['ReWeight'], errors='coerce').fillna(1.0)\n",
    "                else:\n",
    "                    # If ReWeight wasn't even requested/found, add it with 1.0 for consistency\n",
    "                     df['ReWeight'] = 1.0\n",
    "\n",
    "\n",
    "                return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading tree '{tree_name}' from '{file_path}': {e}\")\n",
    "            # Return empty DataFrame on major error for structure consistency\n",
    "            empty_df = pd.DataFrame()\n",
    "            for var in set(variables + ['runID', 'dy', 'ReWeight']):\n",
    "                 empty_df[var] = pd.Series(dtype='float64')\n",
    "            return empty_df\n",
    "\n",
    "\n",
    "    def add_beam_offset(df):\n",
    "        \"\"\"\n",
    "        Calculates and adds the 'beamOffset' column to the DataFrame based on 'runID'.\n",
    "\n",
    "        Uses a fixed set of run ID ranges to determine the beam offset value for\n",
    "        each event. Events outside the specified ranges will have a beamOffset of 0.0.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Input DataFrame which must contain a 'runID' column.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            The DataFrame with an added 'beamOffset' column. Returns the original\n",
    "            DataFrame with beamOffset set to 0.0 if 'runID' column is missing or all NaN.\n",
    "        \"\"\"\n",
    "        if 'runID' not in df.columns or df['runID'].isnull().all():\n",
    "            print(\"⚠️ Cannot calculate beamOffset: 'runID' column not found or is all NaN. Setting beamOffset to 0.0.\")\n",
    "            df['beamOffset'] = 0.0\n",
    "            return df\n",
    "\n",
    "        # Fill NaN in runID with a value outside all ranges (e.g., -1) before calculation\n",
    "        # This ensures np.select doesn't raise errors on NaN input and maps NaNs to the default value\n",
    "        runids_for_calc = df['runID'].fillna(-1).astype(int)\n",
    "\n",
    "        conditions = [\n",
    "            (runids_for_calc >= 8912) & (runids_for_calc <= 10420),\n",
    "            (runids_for_calc >= 10421) & (runids_for_calc <= 10912),\n",
    "            (runids_for_calc >= 11075) & (runids_for_calc <= 12435),\n",
    "            (runids_for_calc >= 12525) & (runids_for_calc <= 15789),\n",
    "            (runids_for_calc >= 15793) & (runids_for_calc <= 16076)\n",
    "        ]\n",
    "        values = [0.4, 0.4, 1.6, -1.6, -1.6]\n",
    "\n",
    "        # Calculate beamOffset, default to 0.0 if no condition is met (including original NaN runIDs)\n",
    "        df['beamOffset'] = np.select(conditions, values, default=0.0)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    # --- Data ---\n",
    "    try:\n",
    "        data = read_tree(data_file_path, tree_name, variables)\n",
    "        data = add_beam_offset(data) # Calculate beamOffset\n",
    "        # Ensure all variables needed for cuts plus beamOffset and dy exist before applying cuts\n",
    "        # This helps apply_cuts avoid errors for datasets missing specific variables used in cuts\n",
    "        required_for_cuts = list(set(variables + ['beamOffset', 'dy']))\n",
    "        for var in required_for_cuts:\n",
    "             if var not in data.columns:\n",
    "                  data[var] = np.nan # Add any missing required variables\n",
    "        cut_table[\"Data\"] = apply_cuts(data.copy(), cuts, use_weights=False)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing Data file: {e}\")\n",
    "        # Ensure Data column exists even on error for table structure consistency\n",
    "        # Need to know the expected cut names to create a Series with zeros\n",
    "        expected_cut_names = [\"Total Events\"] + list(cuts.keys())\n",
    "        cut_table[\"Data\"] = pd.Series(0.0, index=expected_cut_names)\n",
    "\n",
    "\n",
    "    # --- Data(RS67) ---\n",
    "    # Assuming 'result' tree also has runID and dy\n",
    "    try:\n",
    "        rs67_data = read_tree(mixed_file_path, \"result\", variables)\n",
    "        rs67_data = add_beam_offset(rs67_data) # Calculate beamOffset\n",
    "        # Ensure all cut variables plus beamOffset and dy exist\n",
    "        required_for_cuts = list(set(variables + ['beamOffset', 'dy']))\n",
    "        for var in required_for_cuts:\n",
    "             if var not in rs67_data.columns:\n",
    "                  rs67_data[var] = np.nan # Add any missing required variables\n",
    "        cut_table[\"Data(RS67)\"] = apply_cuts(rs67_data.copy(), cuts, use_weights=False)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing RS67 data: {e}\")\n",
    "        expected_cut_names = [\"Total Events\"] + list(cuts.keys())\n",
    "        cut_table[\"Data(RS67)\"] = pd.Series(0.0, index=expected_cut_names)\n",
    "\n",
    "\n",
    "    # --- Mixed(RS67) ---\n",
    "    # Assuming mixed_tree also has runID and dy\n",
    "    try:\n",
    "        mixed_data = read_tree(mixed_file_path, mixed_tree_name, variables)\n",
    "        mixed_data = add_beam_offset(mixed_data) # Calculate beamOffset\n",
    "        # Ensure all cut variables plus beamOffset and dy exist\n",
    "        required_for_cuts = list(set(variables + ['beamOffset', 'dy']))\n",
    "        for var in required_for_cuts:\n",
    "             if var not in mixed_data.columns:\n",
    "                  mixed_data[var] = np.nan # Add any missing required variables\n",
    "        cut_table[\"Mixed(RS67)\"] = apply_cuts(mixed_data.copy(), cuts, use_weights=False)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing Mixed file: {e}\")\n",
    "        expected_cut_names = [\"Total Events\"] + list(cuts.keys())\n",
    "        cut_table[\"Mixed(RS67)\"] = pd.Series(0.0, index=expected_cut_names)\n",
    "\n",
    "\n",
    "    # --- MC files ---\n",
    "    mc_results = {} # Store individual MC results Series by label\n",
    "    for i, mc_file_path in enumerate(mc_file_paths):\n",
    "        label = mc_labels[i]\n",
    "        try:\n",
    "            # Read variables + ReWeight + runID + dy for MC\n",
    "            mc_data = read_tree(mc_file_path, tree_name, variables + [\"ReWeight\"])\n",
    "            mc_data = add_beam_offset(mc_data) # Calculate beamOffset\n",
    "             # Ensure all cut variables plus beamOffset and dy exist\n",
    "            required_for_cuts = list(set(variables + ['beamOffset', 'dy']))\n",
    "            for var in required_for_cuts:\n",
    "                if var not in mc_data.columns:\n",
    "                     mc_data[var] = np.nan # Add any missing required variables\n",
    "\n",
    "            mc_results[label] = apply_cuts(mc_data.copy(), cuts, use_weights=True)\n",
    "            cut_table[label] = mc_results[label] # Add to cut_table immediately\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing MC file '{label}': {e}\")\n",
    "            # Ensure MC column exists even on error\n",
    "            expected_cut_names = [\"Total Events\"] + list(cuts.keys())\n",
    "            cut_table[label] = pd.Series(0.0, index=expected_cut_names)\n",
    "            mc_results[label] = pd.Series(0.0, index=expected_cut_names) # Add zero series for summation\n",
    "\n",
    "\n",
    "    # --- Total MC ---\n",
    "    try:\n",
    "         # Sum the results from the mc_results dictionary\n",
    "         if mc_results:\n",
    "             # Ensure all Series in mc_results have the same index before summing\n",
    "             # This handles cases where one dataset failed and returned a Series with zeros\n",
    "             all_mc_series = [s for s in mc_results.values()]\n",
    "             if all_mc_series:\n",
    "                 # Align indices - fill missing cuts with 0 for summation\n",
    "                 cut_table[\"Total MC\"] = pd.concat(all_mc_series, axis=1).sum(axis=1, min_count=1).fillna(0)\n",
    "             else:\n",
    "                  cut_table[\"Total MC\"] = pd.Series(0.0, index=[\"Total Events\"] + list(cuts.keys()))\n",
    "\n",
    "         else:\n",
    "             cut_table[\"Total MC\"] = pd.Series(0.0, index=[\"Total Events\"] + list(cuts.keys()))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error calculating Total MC: {e}\")\n",
    "        cut_table[\"Total MC\"] = pd.Series(0.0, index=[\"Total Events\"] + list(cuts.keys()))\n",
    "\n",
    "\n",
    "    # --- Purity & Efficiency for DY MC ---\n",
    "    try:\n",
    "        dy_col = \"DY MC\"\n",
    "        # Ensure columns and index exist before calculation\n",
    "        if dy_col in cut_table.columns and \"Total Events\" in cut_table.index and \"Total MC\" in cut_table.columns:\n",
    "             total_dy = cut_table.loc[\"Total Events\", dy_col]\n",
    "             # Avoid division by zero, handle inf/nan results\n",
    "             # Use .copy() here to prevent SettingWithCopyWarning if modifying in place later\n",
    "             # Ensure divisor is not zero for purity\n",
    "             total_mc_safe = cut_table[\"Total MC\"].replace(0, np.nan) # Replace 0 with NaN for division\n",
    "             purity = (cut_table[dy_col] / total_mc_safe).replace([np.inf, -np.inf], np.nan).fillna(0).copy()\n",
    "\n",
    "             # Ensure total_dy is not zero for efficiency\n",
    "             total_dy_safe = total_dy if total_dy != 0 else np.nan\n",
    "             efficiency = (cut_table[dy_col] / total_dy_safe).replace([np.inf, -np.inf], np.nan).fillna(0).copy()\n",
    "\n",
    "\n",
    "             cut_table[\"Purity (DY MC)\"] = purity * 100\n",
    "             cut_table[\"Efficiency (DY MC)\"] = efficiency * 100\n",
    "        else:\n",
    "            print(f\"⚠️ Cannot calculate Purity/Efficiency: Missing '{dy_col}' column, 'Total Events' index, or 'Total MC' column. Or Total Events for DY MC is zero.\")\n",
    "            cut_table[\"Purity (DY MC)\"] = 0.0\n",
    "            cut_table[\"Efficiency (DY MC)\"] = 0.0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error calculating Purity/Efficiency: {e}\")\n",
    "        cut_table[\"Purity (DY MC)\"] = 0.0\n",
    "        cut_table[\"Efficiency (DY MC)\"] = 0.0\n",
    "\n",
    "\n",
    "    cut_table.index.name = \"Cut Name\"\n",
    "\n",
    "    # Ensure \"Total Events\" row is the first index and all cuts from cuts_dict are present\n",
    "    expected_indices = [\"Total Events\"] + list(cuts.keys())\n",
    "    # Reindex the DataFrame to ensure the correct order and presence of all cuts (filling missing rows with 0.0)\n",
    "    cut_table = cut_table.reindex(expected_indices, fill_value=0.0)\n",
    "\n",
    "\n",
    "    return cut_table\n",
    "\n",
    "\n",
    "# ------------------- Configuration -------------------\n",
    "\n",
    "cuts_dict = {\n",
    "    \"nhits1 + nhits2 > 29\": \"(nHits1 + nHits2) > 29\",\n",
    "    \"nhits1st1 + nhits2st1 > 8\": \"(nHits1St1 + nHits2St1) > 8\",\n",
    "    \"chisq dimuon < 18\": \"chisq_dimuon < 18\",\n",
    "    \"chisq Target within 2\": \"abs(chisq1_target + chisq2_target - chisq_dimuon) < 2\",\n",
    "    \"dx within -0.25 to 0.25\": \"dx > -0.25 and dx < 0.25\",\n",
    "    \"dz within -280 to -5\": \"dz > -280 and dz < -5\",\n",
    "    # Re-adding the dy - beamOffset cut\n",
    "    #\"dy - beamOffset within -0.22 to 0.22\": \"(dy - beamOffset) > -0.22 and (dy - beamOffset) < 0.22\",\n",
    "    #\"dy - beamOffset within -0.22 to 0.22\": \"(abs(dy-1.6) < 0.22 and runID > 11000) or (abs(dy-0.4) < 0.22 and runID < 11000)\", # Example alternative\n",
    "    #\"dx^2 + (dy - beamOffset)^2 < 0.06\":\"((dx*dx+(dy-1.6)*(dy-1.6)<0.06 and runID > 11000) or (dx*dx+(dy-0.4)*(dy-0.4)<0.06 and runID < 11000))\", # Example alternative\n",
    "    \"abs(trackSeparation) < 270\": \"abs(trackSeparation) < 270\",\n",
    "    \"abs(dpx) < 1.8\": \"abs(dpx) < 1.8\",\n",
    "    \"abs(dpy) < 2\": \"abs(dpy) < 2\",\n",
    "    \"dpz within 38 to 116\": \"dpz > 38 and dpz < 116\",\n",
    "    \"dpx^2 + dpy^2 < 5\": \"(dpx**2 + dpy**2) < 5\",\n",
    "    \"mass within 4.2 to 8.8\": \"mass > 4.2 and mass < 8.8\",\n",
    "    \"xF within -0.1 to 0.95\": \"xF > -0.1 and xF < 0.95\",\n",
    "    \"xT within -0.1 to 0.58\": \"xT > -0.1 and xT < 0.58\",\n",
    "    \"cosTheta within -0.5 to 0.5\": \"costh > -0.5 and costh < 0.5\",\n",
    "    \"D1 < 400\": \"D1 < 400\",\n",
    "    \"D2 < 400\": \"D2 < 400\",\n",
    "    \"D3 < 400\": \"D3 < 400\",\n",
    "    \"D1 + D2 + D3 < 1000\": \"D1 + D2 + D3 < 1000\",\n",
    "    \"intensity within 0 to 80000\": \"intensityP > 0 and intensityP < 80000\"\n",
    "}\n",
    "\n",
    "variables_list = [\n",
    "    \"nHits1\", \"nHits2\", \"nHits1St1\", \"nHits2St1\", \"chisq_dimuon\", \"chisq1_target\", \"chisq2_target\",\n",
    "    \"dx\", \"dy\", \"dz\", \"dpx\", \"dpy\", \"dpz\", \"mass\", \"D1\", \"D2\", \"D3\", \"xF\", \"xT\", \"xB\", \"costh\", \"intensityP\",\n",
    "    \"runID\", \"trackSeparation\" # <--- Need runID for beamOffset calculation, trackSeparation added from cuts_dict\n",
    "    # 'beamOffset' is not in ROOT files, it's calculated, so don't add it here.\n",
    "    # 'ReWeight' is handled specifically in read_tree\n",
    "]\n",
    "\n",
    "data_file = \"../../ROOTFiles/Hugo/roadset57_70_R008_2111v42_tmp_noPhys.root\"\n",
    "\n",
    "mc_files = [\n",
    "    \"../../ROOTFiles/Hugo/mc_drellyan_LH2_M027_S001_messy_occ_pTxFweight_v2.root\",\n",
    "    \"../../ROOTFiles/Hugo/mc_jpsi_LH2_M027_S001_messy_occ_pTxFweight_v2.root\",\n",
    "    \"../../ROOTFiles/Hugo/mc_psiprime_LH2_M027_S001_messy_occ_pTxFweight_v2.root\",\n",
    "]\n",
    "\n",
    "mc_labels_list = [\"DY MC\", \"J/Psi MC\", \"Psi Prime MC\"]\n",
    "\n",
    "tree = \"Tree\"\n",
    "mixed_file = \"../../ROOTFiles/MixedEvents/merged_RS67_3089LH2.root\"\n",
    "mixed_tree = \"result_mix\"\n",
    "\n",
    "# ------------------- Generate Cut Table -------------------\n",
    "\n",
    "cut_table_df = create_cut_table(data_file, mc_files, mc_labels_list, tree, cuts_dict, variables_list, mixed_file, mixed_tree)\n",
    "\n",
    "if cut_table_df is not None:\n",
    "    display_df = cut_table_df.copy()\n",
    "\n",
    "    # Ensure \"Total Events\" is the first cut name and all cuts from cuts_dict are included\n",
    "    cut_names_order = [\"Total Events\"] + list(cuts_dict.keys())\n",
    "    # Reindex display_df to match the desired order and include all cut names\n",
    "    display_df = display_df.reindex(cut_names_order)\n",
    "\n",
    "\n",
    "    display_df.insert(0, \"Cut\", display_df.index) # Use the index for the 'Cut' column\n",
    "\n",
    "    # Define columns that should be formatted as integers (event counts)\n",
    "    count_cols = [\"Data\", \"Data(RS67)\", \"Mixed(RS67)\", \"Total MC\"] + mc_labels_list\n",
    "    for col in count_cols:\n",
    "        if col in display_df.columns:\n",
    "            # Use .round(0) and .astype(int) to ensure they are integers\n",
    "            display_df[col] = pd.to_numeric(display_df[col], errors=\"coerce\").fillna(0).round(0).astype(int)\n",
    "\n",
    "\n",
    "    # Define a function to format based on the column\n",
    "    def format_value(value, col_name):\n",
    "        if col_name in count_cols:\n",
    "            return \"{:.0f}\".format(value)\n",
    "        elif col_name in [\"Purity (DY MC)\", \"Efficiency (DY MC)\"]: # Percentage columns\n",
    "             # Handle potential non-numeric after fillna(0) if source was bad\n",
    "             try:\n",
    "                 return \"{:.2f}\".format(float(value))\n",
    "             except (ValueError, TypeError):\n",
    "                  return str(value) # Fallback if conversion fails\n",
    "        else:\n",
    "             # Default formatting for unexpected columns\n",
    "             return str(value)\n",
    "\n",
    "    # Apply the formatting row by row\n",
    "    # Use display_df.columns to get columns excluding the 'Cut' column we just inserted\n",
    "    data_to_display = []\n",
    "    for index, row in display_df.iterrows():\n",
    "        formatted_row = [row[\"Cut\"]] + [format_value(row[col], col) for col in display_df.columns[1:]]\n",
    "        data_to_display.append(formatted_row)\n",
    "\n",
    "\n",
    "    print(tabulate(data_to_display, headers=[\"Cut\"] + list(display_df.columns[1:]), tablefmt=\"fancy_grid\"))\n",
    "    cut_table_df.to_csv(\"cut_table_with_mixed_and_reweighting.csv\")\n",
    "    print(\"\\n✅ Cut table saved as 'cut_table_with_mixed_and_reweighting.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44a1eb5",
   "metadata": {},
   "source": [
    "## 1.3 Questions to Steve:\n",
    "### 1.3.1 Why it is not possible to apply beam offset cuts to both data and MC events (Why am I getting 0s for MC columns)?\n",
    "### 1.3.1 What is the correct procedure to create cut table with beam related cuts?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fa220a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63512d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Missing variables in ../../ROOTFiles/MixedEvents/merged_RS67_3089LH2.root: ['intensityP']. These will be added as NaN.\n",
      "⚠️ Missing variables in ../../ROOTFiles/MixedEvents/merged_RS67_3089LH2.root: ['intensityP']. These will be added as NaN.\n",
      "⚠️ Missing variables in ../../ROOTFiles/Hugo/mc_drellyan_LH2_M027_S001_messy_occ_pTxFweight_v2.root: ['intensityP']. These will be added as NaN.\n",
      "⚠️ Missing variables in ../../ROOTFiles/Hugo/mc_jpsi_LH2_M027_S001_messy_occ_pTxFweight_v2.root: ['intensityP']. These will be added as NaN.\n",
      "⚠️ Missing variables in ../../ROOTFiles/Hugo/mc_psiprime_LH2_M027_S001_messy_occ_pTxFweight_v2.root: ['intensityP']. These will be added as NaN.\n",
      "╒══════════════════════════════════════╤════════╤══════════════╤═══════════════╤══════════╤════════════╤════════════════╤════════════╤══════════════════╤══════════════════════╕\n",
      "│ Cut                                  │   Data │   Data(RS67) │   Mixed(RS67) │    DY MC │   J/Psi MC │   Psi Prime MC │   Total MC │   Purity (DY MC) │   Efficiency (DY MC) │\n",
      "╞══════════════════════════════════════╪════════╪══════════════╪═══════════════╪══════════╪════════════╪════════════════╪════════════╪══════════════════╪══════════════════════╡\n",
      "│ Total Events                         │ 614663 │     16517271 │       7945229 │ 19093900 │   60526336 │        6666208 │   86286448 │            22.13 │               100    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ nhits1 + nhits2 > 29                 │ 614663 │     16191722 │       7768630 │ 18821968 │   59839112 │        6596496 │   85257576 │            22.08 │                98.58 │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ nhits1st1 + nhits2st1 > 8            │ 614663 │     15936907 │       7653785 │ 18694676 │   59586104 │        6570602 │   84851384 │            22.03 │                97.91 │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ chisq dimuon < 18                    │ 614663 │     10636888 │       4752043 │ 17291216 │   56534992 │        6246022 │   80072232 │            21.59 │                90.56 │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ chisq Target within 2                │ 614663 │       412300 │        192954 │ 14880651 │   41357444 │        5287561 │   61525656 │            24.19 │                77.93 │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ dx within -0.25 to 0.25              │ 614663 │       314041 │        131027 │ 14648834 │   39442396 │        5210330 │   59301564 │            24.7  │                76.72 │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ dz within -280 to -5                 │ 614663 │       308081 │        127346 │ 14642167 │   39418396 │        5206895 │   59267460 │            24.71 │                76.69 │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ dy - beamOffset within -0.22 to 0.22 │ 614663 │       278763 │        107305 │        0 │          4 │              0 │          4 │             0    │                 0    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ dx^2 + (dy - beamOffset)^2 < 0.06    │ 614663 │       245690 │         89542 │        0 │          4 │              0 │          4 │             0    │                 0    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ abs(trackSeparation) < 270           │ 614663 │       229892 │         80114 │        0 │          0 │              0 │          0 │             0    │                 0    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ abs(dpx) < 1.8                       │ 614663 │       213823 │         71385 │        0 │          0 │              0 │          0 │             0    │                 0    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ abs(dpy) < 2                         │ 614663 │       209938 │         69414 │        0 │          0 │              0 │          0 │             0    │                 0    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ dpz within 38 to 116                 │ 614663 │       194007 │         57478 │        0 │          0 │              0 │          0 │             0    │                 0    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ dpx^2 + dpy^2 < 5                    │ 614663 │       192915 │         56895 │        0 │          0 │              0 │          0 │             0    │                 0    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ mass within 4.2 to 8.8               │ 143661 │        42283 │         11708 │        0 │          0 │              0 │          0 │             0    │                 0    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ xF within -0.1 to 0.95               │ 143309 │        41728 │         11474 │        0 │          0 │              0 │          0 │             0    │                 0    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ xT within -0.1 to 0.58               │ 143309 │        41728 │         11473 │        0 │          0 │              0 │          0 │             0    │                 0    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ cosTheta within -0.5 to 0.5          │ 129477 │        30450 │          5815 │        0 │          0 │              0 │          0 │             0    │                 0    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ D1 < 400                             │ 129236 │        30400 │          5790 │        0 │          0 │              0 │          0 │             0    │                 0    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ D2 < 400                             │ 129186 │        30399 │          5790 │        0 │          0 │              0 │          0 │             0    │                 0    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ D3 < 400                             │ 128928 │        30382 │          5787 │        0 │          0 │              0 │          0 │             0    │                 0    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ D1 + D2 + D3 < 1000                  │ 128715 │        30354 │          5779 │        0 │          0 │              0 │          0 │             0    │                 0    │\n",
      "├──────────────────────────────────────┼────────┼──────────────┼───────────────┼──────────┼────────────┼────────────────┼────────────┼──────────────────┼──────────────────────┤\n",
      "│ intensity within 0 to 80000          │ 128631 │            0 │             0 │        0 │          0 │              0 │          0 │             0    │                 0    │\n",
      "╘══════════════════════════════════════╧════════╧══════════════╧═══════════════╧══════════╧════════════╧════════════════╧════════════╧══════════════════╧══════════════════════╛\n",
      "\n",
      "✅ Cut table saved as 'cut_table_with_mixed_and_reweighting.csv'\n"
     ]
    }
   ],
   "source": [
    "import uproot\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "\n",
    "def create_cut_table(data_file_path, mc_file_paths, mc_labels, tree_name, cuts, variables, mixed_file_path, mixed_tree_name):\n",
    "    \"\"\"\n",
    "    Creates a cut flow table by applying sequential cuts to data, MC, and mixed event samples.\n",
    "\n",
    "    Reads data from ROOT files, calculates a run-dependent beam offset, applies a\n",
    "    series of cuts, and reports the number of events remaining after each cut.\n",
    "    Includes calculations for total MC, purity, and efficiency for the DY MC sample.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_file_path : str\n",
    "        Path to the data ROOT file.\n",
    "    mc_file_paths : list of str\n",
    "        List of paths to the MC ROOT files.\n",
    "    mc_labels : list of str\n",
    "        List of labels corresponding to the MC files (used for column names).\n",
    "    tree_name : str\n",
    "        Name of the tree within the ROOT files containing the event data\n",
    "        (used for data and MC files).\n",
    "    cuts : dict\n",
    "        A dictionary where keys are descriptive cut names and values are\n",
    "        string expressions parsable by pandas.DataFrame.eval(). Cuts are applied\n",
    "        sequentially in the order they appear in the dictionary.\n",
    "    variables : list of str\n",
    "        List of variable names to read from the ROOT trees. This list should\n",
    "        include all variables used in the cut expressions, plus 'runID' and 'dy'.\n",
    "    mixed_file_path : str\n",
    "        Path to the file containing both Data(RS67) and Mixed(RS67) samples.\n",
    "    mixed_tree_name : str\n",
    "        Name of the tree within the mixed file containing the Mixed(RS67) data.\n",
    "        Assumes the Data(RS67) is in a tree named \"result\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or None\n",
    "        A pandas DataFrame representing the cut flow table with cut names as index\n",
    "        and samples (Data, MCs, Total MC, Mixed, Purity, Efficiency) as columns.\n",
    "        Returns None if a critical error occurs during data processing.\n",
    "    \"\"\"\n",
    "    cut_table = pd.DataFrame()\n",
    "\n",
    "    def apply_cuts(df, cuts, use_weights=True):\n",
    "        \"\"\"\n",
    "        Applies a dictionary of sequential cuts to a pandas DataFrame.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Input DataFrame containing event data.\n",
    "        cuts : dict\n",
    "            A dictionary of cut names and their corresponding string expressions.\n",
    "            Cuts are applied sequentially.\n",
    "        use_weights : bool, optional\n",
    "            Whether to use the 'ReWeight' column for summing events. Defaults to True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.Series\n",
    "            A pandas Series where the index is the cut name (including \"Total Events\")\n",
    "            and the value is the sum of event weights (or counts if no weights)\n",
    "            remaining after applying the cut and all previous cuts.\n",
    "        \"\"\"\n",
    "        cut_results = {}\n",
    "        # Use 'ReWeight' column if it exists and use_weights is True, otherwise use count of 1\n",
    "        weight = df['ReWeight'] if use_weights and 'ReWeight' in df.columns else pd.Series(1.0, index=df.index)\n",
    "\n",
    "        # Ensure weight is numeric\n",
    "        weight = pd.to_numeric(weight, errors='coerce').fillna(1.0)\n",
    "\n",
    "        cut_results[\"Total Events\"] = weight.sum()\n",
    "\n",
    "        # Ensure beamOffset column exists before cuts that might use it\n",
    "        if 'beamOffset' not in df.columns:\n",
    "             df['beamOffset'] = 0.0 # Default to 0 if calculation failed or column is missing\n",
    "\n",
    "        # Ensure dy is numeric before cuts that use it\n",
    "        if 'dy' in df.columns:\n",
    "             df['dy'] = pd.to_numeric(df['dy'], errors='coerce').fillna(np.nan)\n",
    "\n",
    "        # Keep track of the mask from successful cuts\n",
    "        cumulative_mask = pd.Series(True, index=df.index)\n",
    "\n",
    "\n",
    "        for cut_name, cut_string in cuts.items():\n",
    "            # Skip the total events entry if it somehow ended up in cuts\n",
    "            if cut_name == \"Total Events\":\n",
    "                 continue\n",
    "            try:\n",
    "                # Evaluate the cut string on the *current* filtered DataFrame\n",
    "                mask = df.eval(cut_string, engine='python')\n",
    "\n",
    "                # Apply the mask to the cumulative mask and the original weight series\n",
    "                cumulative_mask = cumulative_mask & mask\n",
    "\n",
    "                # Calculate the number of events remaining after this cut (using original weights)\n",
    "                cut_results[cut_name] = weight[cumulative_mask].sum()\n",
    "\n",
    "                # Filter the DataFrame and weight series for the *next* iteration\n",
    "                # We filter df here so subsequent cuts are applied to the reduced dataset\n",
    "                df = df[mask].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "                # Note: weight is *not* filtered here; we apply the cumulative_mask to the *original* weight series later\n",
    "                # This is important for correct sequential summing. The filtered 'df' is used for the *next* df.eval()\n",
    "\n",
    "            except Exception as e:\n",
    "                # This catch block now handles actual errors from df.eval(),\n",
    "                # e.g., if a column name used in the string is missing\n",
    "                print(f\"⚠️ Error applying cut '{cut_name}': {e}. Skipping cut for this dataset.\")\n",
    "                cut_results[cut_name] = 0\n",
    "                # If a cut fails, the cumulative_mask is NOT updated by this cut's mask.\n",
    "                # The dataframe 'df' is also NOT filtered by the failed mask.\n",
    "                # The next cut will be applied to the dataframe filtered only by *previous successful* cuts.\n",
    "\n",
    "\n",
    "        return pd.Series(cut_results)\n",
    "\n",
    "\n",
    "    def read_tree(file_path, tree_name, variables):\n",
    "        \"\"\"\n",
    "        Reads specified variables from a ROOT tree into a pandas DataFrame.\n",
    "\n",
    "        Handles missing files, trees, and variables gracefully by returning an empty\n",
    "        DataFrame on major errors or adding missing columns with NaN. Ensures\n",
    "        essential columns ('runID', 'dy', 'ReWeight') are always attempted and\n",
    "        converted to numeric types.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path : str\n",
    "            Path to the ROOT file.\n",
    "        tree_name : str\n",
    "            Name of the tree within the file.\n",
    "        variables : list of str\n",
    "            List of variable names to read.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            A pandas DataFrame containing the requested variables, plus 'runID', 'dy',\n",
    "            and 'ReWeight' if available. Missing columns are added as NaN. Returns\n",
    "            an empty DataFrame if the file or tree cannot be read.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with uproot.open(file_path) as file:\n",
    "                if tree_name not in file:\n",
    "                    print(f\"❌ Error: Tree '{tree_name}' not found in file '{file_path}'.\")\n",
    "                    # Return empty DataFrame with relevant columns for downstream processing structure\n",
    "                    empty_df = pd.DataFrame()\n",
    "                    # Include potentially required columns in empty df\n",
    "                    for var in set(variables + ['runID', 'dy', 'ReWeight']):\n",
    "                         empty_df[var] = pd.Series(dtype='float64') # Use float to accommodate NaN\n",
    "                    return empty_df\n",
    "\n",
    "\n",
    "                tree = file[tree_name]\n",
    "                all_keys = tree.keys()\n",
    "\n",
    "                # Ensure variables needed for beamOffset calculation and the cuts are requested\n",
    "                # Use a set to avoid duplicates, then convert back to list\n",
    "                vars_to_request = list(set(variables + ['runID', 'dy', 'ReWeight']))\n",
    "\n",
    "                vars_to_read = [var for var in vars_to_request if var in all_keys]\n",
    "                missing = [var for var in vars_to_request if var not in all_keys]\n",
    "                if missing:\n",
    "                    # Filter out 'ReWeight' from missing warning if it's likely data\n",
    "                    is_mc_file = any(mc_f in file_path for mc_f in mc_files) # Basic check\n",
    "                    if 'ReWeight' in missing and not is_mc_file:\n",
    "                         missing.remove('ReWeight')\n",
    "                         if missing:\n",
    "                             print(f\"⚠️ Missing variables in {file_path}: {missing}. These will be added as NaN.\")\n",
    "                         # else: print(f\"✅ All expected variables except ReWeight found in {file_path} (assuming data).\") # Optional verbose\n",
    "                    elif missing:\n",
    "                         print(f\"⚠️ Missing variables in {file_path}: {missing}. These will be added as NaN.\")\n",
    "\n",
    "\n",
    "                df = tree.arrays(vars_to_read, library=\"pd\")\n",
    "\n",
    "                # Add missing columns with NaN values to ensure DataFrame has expected structure\n",
    "                for var in missing:\n",
    "                     if var not in df.columns: # Avoid adding if it somehow got added during read\n",
    "                         df[var] = np.nan\n",
    "\n",
    "                # Ensure runID, dy, and ReWeight are numeric (important for beamOffset, cuts, and weights)\n",
    "                if 'runID' in df.columns:\n",
    "                    df['runID'] = pd.to_numeric(df['runID'], errors='coerce')\n",
    "                if 'dy' in df.columns:\n",
    "                    df['dy'] = pd.to_numeric(df['dy'], errors='coerce')\n",
    "                if 'ReWeight' in df.columns:\n",
    "                    # Fill NaN weights with 1.0 (no reweighting)\n",
    "                    df['ReWeight'] = pd.to_numeric(df['ReWeight'], errors='coerce').fillna(1.0)\n",
    "                else:\n",
    "                    # If ReWeight wasn't even requested/found, add it with 1.0 for consistency\n",
    "                     df['ReWeight'] = 1.0\n",
    "\n",
    "\n",
    "                return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading tree '{tree_name}' from '{file_path}': {e}\")\n",
    "            # Return empty DataFrame on major error for structure consistency\n",
    "            empty_df = pd.DataFrame()\n",
    "            for var in set(variables + ['runID', 'dy', 'ReWeight']):\n",
    "                 empty_df[var] = pd.Series(dtype='float64')\n",
    "            return empty_df\n",
    "\n",
    "\n",
    "    def add_beam_offset(df):\n",
    "        \"\"\"\n",
    "        Calculates and adds the 'beamOffset' column to the DataFrame based on 'runID'.\n",
    "\n",
    "        Uses a fixed set of run ID ranges to determine the beam offset value for\n",
    "        each event. Events outside the specified ranges will have a beamOffset of 0.0.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Input DataFrame which must contain a 'runID' column.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            The DataFrame with an added 'beamOffset' column. Returns the original\n",
    "            DataFrame with beamOffset set to 0.0 if 'runID' column is missing or all NaN.\n",
    "        \"\"\"\n",
    "        if 'runID' not in df.columns or df['runID'].isnull().all():\n",
    "            print(\"⚠️ Cannot calculate beamOffset: 'runID' column not found or is all NaN. Setting beamOffset to 0.0.\")\n",
    "            df['beamOffset'] = 0.0\n",
    "            return df\n",
    "\n",
    "        # Fill NaN in runID with a value outside all ranges (e.g., -1) before calculation\n",
    "        # This ensures np.select doesn't raise errors on NaN input and maps NaNs to the default value\n",
    "        runids_for_calc = df['runID'].fillna(-1).astype(int)\n",
    "\n",
    "        conditions = [\n",
    "            (runids_for_calc >= 8912) & (runids_for_calc <= 10420),\n",
    "            (runids_for_calc >= 10421) & (runids_for_calc <= 10912),\n",
    "            (runids_for_calc >= 11075) & (runids_for_calc <= 12435),\n",
    "            (runids_for_calc >= 12525) & (runids_for_calc <= 15789),\n",
    "            (runids_for_calc >= 15793) & (runids_for_calc <= 16076)\n",
    "        ]\n",
    "        values = [0.4, 0.4, 1.6, -1.6, -1.6]\n",
    "\n",
    "        # Calculate beamOffset, default to 0.0 if no condition is met (including original NaN runIDs)\n",
    "        df['beamOffset'] = np.select(conditions, values, default=0.0)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    # --- Data ---\n",
    "    try:\n",
    "        data = read_tree(data_file_path, tree_name, variables)\n",
    "        data = add_beam_offset(data) # Calculate beamOffset\n",
    "        # Ensure all variables needed for cuts plus beamOffset and dy exist before applying cuts\n",
    "        # This helps apply_cuts avoid errors for datasets missing specific variables used in cuts\n",
    "        required_for_cuts = list(set(variables + ['beamOffset', 'dy']))\n",
    "        for var in required_for_cuts:\n",
    "             if var not in data.columns:\n",
    "                  data[var] = np.nan # Add any missing required variables\n",
    "        cut_table[\"Data\"] = apply_cuts(data.copy(), cuts, use_weights=False)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing Data file: {e}\")\n",
    "        # Ensure Data column exists even on error for table structure consistency\n",
    "        # Need to know the expected cut names to create a Series with zeros\n",
    "        expected_cut_names = [\"Total Events\"] + list(cuts.keys())\n",
    "        cut_table[\"Data\"] = pd.Series(0.0, index=expected_cut_names)\n",
    "\n",
    "\n",
    "    # --- Data(RS67) ---\n",
    "    # Assuming 'result' tree also has runID and dy\n",
    "    try:\n",
    "        rs67_data = read_tree(mixed_file_path, \"result\", variables)\n",
    "        rs67_data = add_beam_offset(rs67_data) # Calculate beamOffset\n",
    "        # Ensure all cut variables plus beamOffset and dy exist\n",
    "        required_for_cuts = list(set(variables + ['beamOffset', 'dy']))\n",
    "        for var in required_for_cuts:\n",
    "             if var not in rs67_data.columns:\n",
    "                  rs67_data[var] = np.nan # Add any missing required variables\n",
    "        cut_table[\"Data(RS67)\"] = apply_cuts(rs67_data.copy(), cuts, use_weights=False)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing RS67 data: {e}\")\n",
    "        expected_cut_names = [\"Total Events\"] + list(cuts.keys())\n",
    "        cut_table[\"Data(RS67)\"] = pd.Series(0.0, index=expected_cut_names)\n",
    "\n",
    "\n",
    "    # --- Mixed(RS67) ---\n",
    "    # Assuming mixed_tree also has runID and dy\n",
    "    try:\n",
    "        mixed_data = read_tree(mixed_file_path, mixed_tree_name, variables)\n",
    "        mixed_data = add_beam_offset(mixed_data) # Calculate beamOffset\n",
    "        # Ensure all cut variables plus beamOffset and dy exist\n",
    "        required_for_cuts = list(set(variables + ['beamOffset', 'dy']))\n",
    "        for var in required_for_cuts:\n",
    "             if var not in mixed_data.columns:\n",
    "                  mixed_data[var] = np.nan # Add any missing required variables\n",
    "        cut_table[\"Mixed(RS67)\"] = apply_cuts(mixed_data.copy(), cuts, use_weights=False)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing Mixed file: {e}\")\n",
    "        expected_cut_names = [\"Total Events\"] + list(cuts.keys())\n",
    "        cut_table[\"Mixed(RS67)\"] = pd.Series(0.0, index=expected_cut_names)\n",
    "\n",
    "\n",
    "    # --- MC files ---\n",
    "    mc_results = {} # Store individual MC results Series by label\n",
    "    for i, mc_file_path in enumerate(mc_file_paths):\n",
    "        label = mc_labels[i]\n",
    "        try:\n",
    "            # Read variables + ReWeight + runID + dy for MC\n",
    "            mc_data = read_tree(mc_file_path, tree_name, variables + [\"ReWeight\"])\n",
    "            mc_data = add_beam_offset(mc_data) # Calculate beamOffset\n",
    "             # Ensure all cut variables plus beamOffset and dy exist\n",
    "            required_for_cuts = list(set(variables + ['beamOffset', 'dy']))\n",
    "            for var in required_for_cuts:\n",
    "                if var not in mc_data.columns:\n",
    "                     mc_data[var] = np.nan # Add any missing required variables\n",
    "\n",
    "            mc_results[label] = apply_cuts(mc_data.copy(), cuts, use_weights=True)\n",
    "            cut_table[label] = mc_results[label] # Add to cut_table immediately\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing MC file '{label}': {e}\")\n",
    "            # Ensure MC column exists even on error\n",
    "            expected_cut_names = [\"Total Events\"] + list(cuts.keys())\n",
    "            cut_table[label] = pd.Series(0.0, index=expected_cut_names)\n",
    "            mc_results[label] = pd.Series(0.0, index=expected_cut_names) # Add zero series for summation\n",
    "\n",
    "\n",
    "    # --- Total MC ---\n",
    "    try:\n",
    "         # Sum the results from the mc_results dictionary\n",
    "         if mc_results:\n",
    "             # Ensure all Series in mc_results have the same index before summing\n",
    "             # This handles cases where one dataset failed and returned a Series with zeros\n",
    "             all_mc_series = [s for s in mc_results.values()]\n",
    "             if all_mc_series:\n",
    "                 # Align indices - fill missing cuts with 0 for summation\n",
    "                 cut_table[\"Total MC\"] = pd.concat(all_mc_series, axis=1).sum(axis=1, min_count=1).fillna(0)\n",
    "             else:\n",
    "                  cut_table[\"Total MC\"] = pd.Series(0.0, index=[\"Total Events\"] + list(cuts.keys()))\n",
    "\n",
    "         else:\n",
    "             cut_table[\"Total MC\"] = pd.Series(0.0, index=[\"Total Events\"] + list(cuts.keys()))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error calculating Total MC: {e}\")\n",
    "        cut_table[\"Total MC\"] = pd.Series(0.0, index=[\"Total Events\"] + list(cuts.keys()))\n",
    "\n",
    "\n",
    "    # --- Purity & Efficiency for DY MC ---\n",
    "    try:\n",
    "        dy_col = \"DY MC\"\n",
    "        # Ensure columns and index exist before calculation\n",
    "        if dy_col in cut_table.columns and \"Total Events\" in cut_table.index and \"Total MC\" in cut_table.columns:\n",
    "             total_dy = cut_table.loc[\"Total Events\", dy_col]\n",
    "             # Avoid division by zero, handle inf/nan results\n",
    "             # Use .copy() here to prevent SettingWithCopyWarning if modifying in place later\n",
    "             # Ensure divisor is not zero for purity\n",
    "             total_mc_safe = cut_table[\"Total MC\"].replace(0, np.nan) # Replace 0 with NaN for division\n",
    "             purity = (cut_table[dy_col] / total_mc_safe).replace([np.inf, -np.inf], np.nan).fillna(0).copy()\n",
    "\n",
    "             # Ensure total_dy is not zero for efficiency\n",
    "             total_dy_safe = total_dy if total_dy != 0 else np.nan\n",
    "             efficiency = (cut_table[dy_col] / total_dy_safe).replace([np.inf, -np.inf], np.nan).fillna(0).copy()\n",
    "\n",
    "\n",
    "             cut_table[\"Purity (DY MC)\"] = purity * 100\n",
    "             cut_table[\"Efficiency (DY MC)\"] = efficiency * 100\n",
    "        else:\n",
    "            print(f\"⚠️ Cannot calculate Purity/Efficiency: Missing '{dy_col}' column, 'Total Events' index, or 'Total MC' column. Or Total Events for DY MC is zero.\")\n",
    "            cut_table[\"Purity (DY MC)\"] = 0.0\n",
    "            cut_table[\"Efficiency (DY MC)\"] = 0.0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error calculating Purity/Efficiency: {e}\")\n",
    "        cut_table[\"Purity (DY MC)\"] = 0.0\n",
    "        cut_table[\"Efficiency (DY MC)\"] = 0.0\n",
    "\n",
    "\n",
    "    cut_table.index.name = \"Cut Name\"\n",
    "\n",
    "    # Ensure \"Total Events\" row is the first index and all cuts from cuts_dict are present\n",
    "    expected_indices = [\"Total Events\"] + list(cuts.keys())\n",
    "    # Reindex the DataFrame to ensure the correct order and presence of all cuts (filling missing rows with 0.0)\n",
    "    cut_table = cut_table.reindex(expected_indices, fill_value=0.0)\n",
    "\n",
    "\n",
    "    return cut_table\n",
    "\n",
    "\n",
    "# ------------------- Configuration -------------------\n",
    "\n",
    "cuts_dict = {\n",
    "    \"nhits1 + nhits2 > 29\": \"(nHits1 + nHits2) > 29\",\n",
    "    \"nhits1st1 + nhits2st1 > 8\": \"(nHits1St1 + nHits2St1) > 8\",\n",
    "    \"chisq dimuon < 18\": \"chisq_dimuon < 18\",\n",
    "    \"chisq Target within 2\": \"abs(chisq1_target + chisq2_target - chisq_dimuon) < 2\",\n",
    "    \"dx within -0.25 to 0.25\": \"dx > -0.25 and dx < 0.25\",\n",
    "    \"dz within -280 to -5\": \"dz > -280 and dz < -5\",\n",
    "    # Re-adding the dy - beamOffset cut\n",
    "    #\"dy - beamOffset within -0.22 to 0.22\": \"(dy - beamOffset) > -0.22 and (dy - beamOffset) < 0.22\",\n",
    "    \"dy - beamOffset within -0.22 to 0.22\": \"(abs(dy-1.6) < 0.22 and runID > 11000) or (abs(dy-0.4) < 0.22 and runID < 11000)\", # Example alternative\n",
    "    \"dx^2 + (dy - beamOffset)^2 < 0.06\":\"((dx*dx+(dy-1.6)*(dy-1.6)<0.06 and runID > 11000) or (dx*dx+(dy-0.4)*(dy-0.4)<0.06 and runID < 11000))\", # Example alternative\n",
    "    \"abs(trackSeparation) < 270\": \"abs(trackSeparation) < 270\",\n",
    "    \"abs(dpx) < 1.8\": \"abs(dpx) < 1.8\",\n",
    "    \"abs(dpy) < 2\": \"abs(dpy) < 2\",\n",
    "    \"dpz within 38 to 116\": \"dpz > 38 and dpz < 116\",\n",
    "    \"dpx^2 + dpy^2 < 5\": \"(dpx**2 + dpy**2) < 5\",\n",
    "    \"mass within 4.2 to 8.8\": \"mass > 4.2 and mass < 8.8\",\n",
    "    \"xF within -0.1 to 0.95\": \"xF > -0.1 and xF < 0.95\",\n",
    "    \"xT within -0.1 to 0.58\": \"xT > -0.1 and xT < 0.58\",\n",
    "    \"cosTheta within -0.5 to 0.5\": \"costh > -0.5 and costh < 0.5\",\n",
    "    \"D1 < 400\": \"D1 < 400\",\n",
    "    \"D2 < 400\": \"D2 < 400\",\n",
    "    \"D3 < 400\": \"D3 < 400\",\n",
    "    \"D1 + D2 + D3 < 1000\": \"D1 + D2 + D3 < 1000\",\n",
    "    \"intensity within 0 to 80000\": \"intensityP > 0 and intensityP < 80000\"\n",
    "}\n",
    "\n",
    "variables_list = [\n",
    "    \"nHits1\", \"nHits2\", \"nHits1St1\", \"nHits2St1\", \"chisq_dimuon\", \"chisq1_target\", \"chisq2_target\",\n",
    "    \"dx\", \"dy\", \"dz\", \"dpx\", \"dpy\", \"dpz\", \"mass\", \"D1\", \"D2\", \"D3\", \"xF\", \"xT\", \"xB\", \"costh\", \"intensityP\",\n",
    "    \"runID\", \"trackSeparation\" # <--- Need runID for beamOffset calculation, trackSeparation added from cuts_dict\n",
    "    # 'beamOffset' is not in ROOT files, it's calculated, so don't add it here.\n",
    "    # 'ReWeight' is handled specifically in read_tree\n",
    "]\n",
    "\n",
    "data_file = \"../../ROOTFiles/Hugo/roadset57_70_R008_2111v42_tmp_noPhys.root\"\n",
    "\n",
    "mc_files = [\n",
    "    \"../../ROOTFiles/Hugo/mc_drellyan_LH2_M027_S001_messy_occ_pTxFweight_v2.root\",\n",
    "    \"../../ROOTFiles/Hugo/mc_jpsi_LH2_M027_S001_messy_occ_pTxFweight_v2.root\",\n",
    "    \"../../ROOTFiles/Hugo/mc_psiprime_LH2_M027_S001_messy_occ_pTxFweight_v2.root\",\n",
    "]\n",
    "\n",
    "mc_labels_list = [\"DY MC\", \"J/Psi MC\", \"Psi Prime MC\"]\n",
    "\n",
    "tree = \"Tree\"\n",
    "mixed_file = \"../../ROOTFiles/MixedEvents/merged_RS67_3089LH2.root\"\n",
    "mixed_tree = \"result_mix\"\n",
    "\n",
    "# ------------------- Generate Cut Table -------------------\n",
    "\n",
    "cut_table_df = create_cut_table(data_file, mc_files, mc_labels_list, tree, cuts_dict, variables_list, mixed_file, mixed_tree)\n",
    "\n",
    "if cut_table_df is not None:\n",
    "    display_df = cut_table_df.copy()\n",
    "\n",
    "    # Ensure \"Total Events\" is the first cut name and all cuts from cuts_dict are included\n",
    "    cut_names_order = [\"Total Events\"] + list(cuts_dict.keys())\n",
    "    # Reindex display_df to match the desired order and include all cut names\n",
    "    display_df = display_df.reindex(cut_names_order)\n",
    "\n",
    "\n",
    "    display_df.insert(0, \"Cut\", display_df.index) # Use the index for the 'Cut' column\n",
    "\n",
    "    # Define columns that should be formatted as integers (event counts)\n",
    "    count_cols = [\"Data\", \"Data(RS67)\", \"Mixed(RS67)\", \"Total MC\"] + mc_labels_list\n",
    "    for col in count_cols:\n",
    "        if col in display_df.columns:\n",
    "            # Use .round(0) and .astype(int) to ensure they are integers\n",
    "            display_df[col] = pd.to_numeric(display_df[col], errors=\"coerce\").fillna(0).round(0).astype(int)\n",
    "\n",
    "\n",
    "    # Define a function to format based on the column\n",
    "    def format_value(value, col_name):\n",
    "        if col_name in count_cols:\n",
    "            return \"{:.0f}\".format(value)\n",
    "        elif col_name in [\"Purity (DY MC)\", \"Efficiency (DY MC)\"]: # Percentage columns\n",
    "             # Handle potential non-numeric after fillna(0) if source was bad\n",
    "             try:\n",
    "                 return \"{:.2f}\".format(float(value))\n",
    "             except (ValueError, TypeError):\n",
    "                  return str(value) # Fallback if conversion fails\n",
    "        else:\n",
    "             # Default formatting for unexpected columns\n",
    "             return str(value)\n",
    "\n",
    "    # Apply the formatting row by row\n",
    "    # Use display_df.columns to get columns excluding the 'Cut' column we just inserted\n",
    "    data_to_display = []\n",
    "    for index, row in display_df.iterrows():\n",
    "        formatted_row = [row[\"Cut\"]] + [format_value(row[col], col) for col in display_df.columns[1:]]\n",
    "        data_to_display.append(formatted_row)\n",
    "\n",
    "\n",
    "    print(tabulate(data_to_display, headers=[\"Cut\"] + list(display_df.columns[1:]), tablefmt=\"fancy_grid\"))\n",
    "    cut_table_df.to_csv(\"cut_table_with_mixed_and_reweighting.csv\")\n",
    "    print(\"\\n✅ Cut table saved as 'cut_table_with_mixed_and_reweighting.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcfe9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
